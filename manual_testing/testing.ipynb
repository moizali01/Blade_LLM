{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pysqlite3\n",
    "\n",
    "sys.modules['sqlite3'] = pysqlite3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import ollama\n",
    "from functools import cached_property\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_text_splitters import (Language,RecursiveCharacterTextSplitter)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import voyageai\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_voyageai import VoyageAIEmbeddings,VoyageAIRerank\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "from chunker import get_code_chunks\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "def doc_merger(splits):\n",
    "    current = 0\n",
    "    while True:\n",
    "        doc_lines = len(splits[current].splitlines())\n",
    "        if doc_lines < 3:\n",
    "            # merge with next doc\n",
    "            splits[current] += splits[current + 1]\n",
    "            splits.pop(current + 1)\n",
    "        else:\n",
    "            current += 1\n",
    "        \n",
    "        if current == len(splits) - 1:\n",
    "            return splits\n",
    "\n",
    "command = [\"clang-format\",\"-style={ColumnLimit: 300, AllowShortFunctionsOnASingleLine: All, AllowShortIfStatementsOnASingleLine: true}\",\"-i\",\"original.txt\"]\n",
    "\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "file_path = \"original.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    docs = f.read()\n",
    "\n",
    "splits = get_code_chunks(docs)\n",
    "new_splits = [split for split in splits if len(split) > 2]\n",
    "new_splits2 = doc_merger(new_splits)\n",
    "# documents = [Document(page_content=split) for split in splits]\n",
    "documents = [Document(page_content=split) for split in new_splits2]\n",
    "# save documents to files\n",
    "# for i, doc in enumerate(documents):\n",
    "#     with open(f\"docs/doc_{i}.txt\", \"w\") as f:\n",
    "#         f.write(doc.page_content)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "db = Chroma.from_documents(documents=documents, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Snippet.{i+1}:\\n\\n{doc.page_content}\" for i, doc in enumerate(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = [\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "]\n",
    "genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "generation_config = {\n",
    "\"temperature\": 0.1,\n",
    "\"top_p\": 0.95,\n",
    "\"top_k\": 64,\n",
    "\"max_output_tokens\": 8192,\n",
    "\"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "model_name=\"gemini-1.5-flash\",\n",
    "generation_config=generation_config,\n",
    "safety_settings = safe\n",
    ")\n",
    "llm = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "def call_retrieval_sada(pretext, fifty_clean):\n",
    "\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    retrieved_docs = retriever.invoke(pretext)\n",
    "\n",
    "    formatted_context = combine_docs(retrieved_docs)\n",
    "\n",
    "    formatted_context_2 = \"Snippet.0: \\n\\n\" + fifty_clean + \"\\n\\n\" + formatted_context\n",
    "\n",
    "    return formatted_context_2\n",
    "\n",
    "    # return formatted_context, [], retrieved_docs, []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your cand sets below\n",
    "\n",
    "##### Add `Pretext`, `Fifty_Text` and `Query` as variables\n",
    "\n",
    "##### Original.txt, prompt, and sec_list must be in `manual_testing/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      " Class 1: This code is not directly related to input preprocessing, parsing, or date calculations. It appears to be involved in output formatting.\n",
      "Class 2: This code might be somewhat unnecessary if the program only supports a limited set of output formats. However, it is likely needed for more complex formatting options.\n",
      "Class 3: This code is necessary for the program to handle different output formatting options, including case sensitivity.\n",
      "Class 4: This code is not critical for the program's core functionality. The program would still function without it, but the output might not be formatted as intended.\n",
      "Class 5: The context provided is sufficient to understand the purpose of this code.\n",
      "Class 6: This code is used in the provided context.\n",
      "\n",
      "Explanation: The code snippet is part of a larger loop that handles output formatting. It checks for flags indicating whether the output should be in lowercase, uppercase, or the original case. This code is necessary for the program to support different output formatting options, but it is not critical for the program's core functionality.\n",
      "\n",
      "Final Verdict: Class 3 (3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = model.start_chat(history=[])\n",
    "\n",
    "with open('prompt_in_coverage.txt', 'r') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "with open('sec_list.txt', 'r') as file:\n",
    "    sec_list = file.read()\n",
    "\n",
    "\n",
    "pretext = \"\"\"while (1) {\n",
    "              if (to_lowcase) {\n",
    "                fwrite_lowcase(p, (char const *)(ubuf + 1), n__2);\n",
    "              } else {\n",
    "                if (to_uppcase) {\n",
    "                  fwrite_uppcase(p, (char const *)(ubuf + 1), n__2);\n",
    "                } else {\n",
    "                  fwrite((void const *)(ubuf + 1), n__2, (size_t)1, p);\n",
    "                }\n",
    "              }\n",
    "              goto while_break___22;\n",
    "            }\n",
    "fputc(' ', p);\n",
    "                      i__6++;\n",
    "                    }\n",
    "                  while_break___21:;\n",
    "                    goto while_break___20;\n",
    "                  }\n",
    "                while_break___20:;\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "            while (1) {\n",
    "\n",
    "              if (to_lowcase) {\n",
    "                fwrite_lowcase(p, (char const *)(ubuf + 1), n__2);\n",
    "              } else {\n",
    "                if (to_uppcase) {\n",
    "                  fwrite_uppcase(p, (char const *)(ubuf + 1), n__2);\n",
    "                } else {\n",
    "                  fwrite((void const *)(ubuf + 1), n__2, (size_t)1, p);\n",
    "                }\n",
    "              }\n",
    "              goto while_break___22;\n",
    "            }\n",
    "          while_break___22:;\n",
    "          }\n",
    "          i += incr__2;\n",
    "          goto while_break___17;\n",
    "        }\n",
    "      while_break___17:;\n",
    "      }\n",
    "      goto switch_break___1;\n",
    "    case_67:\n",
    "      if (modifier == 79) {\n",
    "        goto bad_format;\n",
    "      }\"\"\"\n",
    "\n",
    "fifty_clean = \"\"\"\"\"\"\n",
    "\n",
    "query = \"\"\"while (1) {\n",
    "              if (to_lowcase) {\n",
    "                fwrite_lowcase(p, (char const *)(ubuf + 1), n__2);\n",
    "              } else {\n",
    "                if (to_uppcase) {\n",
    "                  fwrite_uppcase(p, (char const *)(ubuf + 1), n__2);\n",
    "                } else {\n",
    "                  fwrite((void const *)(ubuf + 1), n__2, (size_t)1, p);\n",
    "                }\n",
    "              }\n",
    "              goto while_break___22;\n",
    "            }\"\"\"\n",
    "formatted_context = call_retrieval_sada(pretext, fifty_clean)\n",
    "\n",
    "prompt = prompt_template.format(sec_list=sec_list, formatted_context=formatted_context, query=query)\n",
    "response = llm.send_message(prompt).text\n",
    "\n",
    "print(\"\\nRESPONSE:\\n\",response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpi/.pyenv/versions/3.10.14/envs/base/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpi/.pyenv/versions/3.10.14/envs/base/lib/python3.10/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import ollama\n",
    "from functools import cached_property\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_text_splitters import (Language,RecursiveCharacterTextSplitter)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import voyageai\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_voyageai import VoyageAIEmbeddings,VoyageAIRerank\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "from chunker import get_code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "def doc_merger(splits):\n",
    "    current = 0\n",
    "    while True:\n",
    "        doc_lines = len(splits[current].splitlines())\n",
    "        if doc_lines < 3:\n",
    "            # merge with next doc\n",
    "            splits[current] += splits[current + 1]\n",
    "            splits.pop(current + 1)\n",
    "        else:\n",
    "            current += 1\n",
    "        \n",
    "        if current == len(splits) - 1:\n",
    "            return splits\n",
    "\n",
    "command = [\"clang-format\",\"-style={ColumnLimit: 300, AllowShortFunctionsOnASingleLine: All, AllowShortIfStatementsOnASingleLine: true}\",\"-i\",\"original.txt\"]\n",
    "\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "file_path = \"original.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    docs = f.read()\n",
    "\n",
    "splits = get_code_chunks(docs)\n",
    "new_splits = [split for split in splits if len(split) > 2]\n",
    "new_splits2 = doc_merger(new_splits)\n",
    "# documents = [Document(page_content=split) for split in splits]\n",
    "documents = [Document(page_content=split) for split in new_splits2]\n",
    "# save documents to files\n",
    "# for i, doc in enumerate(documents):\n",
    "#     with open(f\"docs/doc_{i}.txt\", \"w\") as f:\n",
    "#         f.write(doc.page_content)\n",
    "os.environ[\"VOYAGE_API_KEY\"] = os.environ.get(\"VOYAGE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "db = Chroma.from_documents(documents=documents, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Snippet.{i+1}:\\n\\n{doc.page_content}\" for i, doc in enumerate(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = [\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "]\n",
    "genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "generation_config = {\n",
    "\"temperature\": 0.1,\n",
    "\"top_p\": 0.95,\n",
    "\"top_k\": 64,\n",
    "\"max_output_tokens\": 8192,\n",
    "\"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "model_name=\"gemini-1.5-flash\",\n",
    "generation_config=generation_config,\n",
    "safety_settings = safe\n",
    ")\n",
    "llm = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "def call_retrieval_sada(pretext, fifty_clean):\n",
    "\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    retrieved_docs = retriever.invoke(pretext)\n",
    "\n",
    "    formatted_context = combine_docs(retrieved_docs)\n",
    "\n",
    "    formatted_context_2 = \"Snippet.0: \\n\\n\" + fifty_clean + \"\\n\\n\" + formatted_context\n",
    "\n",
    "    return formatted_context_2\n",
    "\n",
    "    # return formatted_context, [], retrieved_docs, []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your cand sets below\n",
    "\n",
    "##### Add `Pretext`, `Fifty_Text` and `Query` as variables\n",
    "\n",
    "##### Original.txt, prompt, and sec_list must be in `manual_testing/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESPONSE:\n",
      " Class 1: This code is not directly involved in date formatting, calculation, input processing, parsing, or output formatting. It does not contribute to any of the required functionalities.\n",
      "Class 2: This code is not directly involved in date formatting, calculation, input processing, parsing, or output formatting. It does not contribute to any of the required functionalities.\n",
      "Class 3: This code is not directly involved in date formatting, calculation, input processing, parsing, or output formatting. It does not contribute to any of the required functionalities.\n",
      "Class 4: This code is a variable declaration, which is crucial for defining the type and size of data, allocating memory, and preventing errors. While its specific purpose in this context is unclear, it is safer to keep it than to remove it.\n",
      "\n",
      "Explanation: The code snippet is a variable declaration, which is essential for program structure and potentially for the overall functionality. However, without more context, it is impossible to determine its exact purpose and how it contributes to the required functionalities. Therefore, it is classified as critical and necessary.\n",
      "\n",
      "Final Verdict: Class 4 (Critical) - I do not fully understand the purpose of this code in the larger context of the program. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = model.start_chat(history=[])\n",
    "\n",
    "with open('single_cand_prompt.txt', 'r') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "with open('sec_list.txt', 'r') as file:\n",
    "    sec_list = file.read()\n",
    "\n",
    "\n",
    "pretext = \"\"\"size_t w__5;\n",
    "    int tmp___19;\n",
    "    size_t incr__5;\n",
    "    size_t tmp___20;\n",
    "    size_t delta__5;\n",
    "    size_t i__12;\n",
    "    size_t i__13;\n",
    "    size_t i__14;\n",
    "    size_t n__6;\n",
    "    size_t w__6;\n",
    "    int tmp___21;\n",
    "    size_t incr__6;\n",
    "    size_t tmp___22;\n",
    "    size_t delta__6;\n",
    "    size_t i__15;\n",
    "    size_t i__16;\n",
    "    size_t n__7;\n",
    "    size_t w__7;\n",
    "    int tmp___23;\n",
    "    size_t incr__7;\"\"\"\n",
    "\n",
    "fifty_clean = \"\"\"size_t w__5;\n",
    "    int tmp___19;\n",
    "    size_t incr__5;\n",
    "    size_t tmp___20;\n",
    "    size_t delta__5;\n",
    "    size_t i__12;\n",
    "    size_t i__13;\n",
    "    size_t i__14;\n",
    "    size_t n__6;\n",
    "    size_t w__6;\n",
    "    int tmp___21;\n",
    "    size_t incr__6;\n",
    "    size_t tmp___22;\n",
    "    size_t delta__6;\n",
    "    size_t i__15;\n",
    "    size_t i__16;\n",
    "    size_t n__7;\n",
    "    size_t w__7;\n",
    "    int tmp___23;\n",
    "    size_t incr__7;\n",
    "    size_t tmp___24;\n",
    "    size_t delta__7;\n",
    "    size_t i__17;\n",
    "    size_t i__18;\n",
    "    int j;\n",
    "    size_t n__8;\n",
    "    size_t w__8;\n",
    "    int tmp___25;\n",
    "    size_t incr__8;\n",
    "    size_t tmp___26;\n",
    "    size_t delta__8;\n",
    "    size_t i__19;\n",
    "    size_t i__20;\n",
    "    struct tm ltm;\n",
    "    time_t t;\n",
    "    int d;\n",
    "    int tmp___27;\n",
    "    size_t n__9;\n",
    "    size_t w__9;\n",
    "    int tmp___28;\n",
    "    size_t incr__9;\n",
    "    size_t tmp___29;\n",
    "    size_t delta__9;\n",
    "    size_t i__21;\n",
    "    size_t i__22;\n",
    "    int year___1;\n",
    "    int tmp___30;\n",
    "    int year_adjust;\n",
    "    int days;\n",
    "    int tmp___31;\n",
    "    int tmp___32;\n",
    "    int d___0;\n",
    "    int tmp___33;\n",
    "    int tmp___34;\n",
    "    int yy;\n",
    "    int tmp___35;\n",
    "    int yy___0;\n",
    "    size_t n__10;\n",
    "    size_t tmp___36;\n",
    "    size_t w__10;\n",
    "    int tmp___37;\n",
    "    size_t incr__10;\n",
    "    size_t tmp___38;\n",
    "    size_t delta__10;\n",
    "    size_t i__23;\n",
    "    size_t i__24;\n",
    "    int diff;\n",
    "    int hour_diff;\n",
    "    int min_diff;\n",
    "    int sec_diff;\n",
    "    int flen;\n",
    "    size_t n__11;\n",
    "    size_t w__11;\n",
    "    int tmp___39;\n",
    "    size_t incr__11;\n",
    "    size_t tmp___40;\n",
    "    size_t delta__11;\n",
    "    size_t i__25;\n",
    "    size_t i__26;\"\"\"\n",
    "\n",
    "query = \"\"\"int tmp___21;\"\"\"\n",
    "formatted_context = call_retrieval_sada(pretext, fifty_clean)\n",
    "\n",
    "prompt = prompt_template.format(sec_list=sec_list, formatted_context=formatted_context, query=query)\n",
    "response = llm.send_message(prompt).text\n",
    "\n",
    "print(\"\\nRESPONSE:\\n\",response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pysqlite3\n",
    "\n",
    "sys.modules['sqlite3'] = pysqlite3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import ollama\n",
    "from functools import cached_property\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_text_splitters import (Language,RecursiveCharacterTextSplitter)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import voyageai\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_voyageai import VoyageAIEmbeddings,VoyageAIRerank\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "from chunker import get_code_chunks\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "def doc_merger(splits):\n",
    "    current = 0\n",
    "    while True:\n",
    "        doc_lines = len(splits[current].splitlines())\n",
    "        if doc_lines < 3:\n",
    "            # merge with next doc\n",
    "            splits[current] += splits[current + 1]\n",
    "            splits.pop(current + 1)\n",
    "        else:\n",
    "            current += 1\n",
    "        \n",
    "        if current == len(splits) - 1:\n",
    "            return splits\n",
    "\n",
    "command = [\"clang-format\",\"-style={ColumnLimit: 300, AllowShortFunctionsOnASingleLine: All, AllowShortIfStatementsOnASingleLine: true}\",\"-i\",\"original.txt\"]\n",
    "\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "file_path = \"original.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    docs = f.read()\n",
    "\n",
    "splits = get_code_chunks(docs)\n",
    "new_splits = [split for split in splits if len(split) > 2]\n",
    "new_splits2 = doc_merger(new_splits)\n",
    "# documents = [Document(page_content=split) for split in splits]\n",
    "documents = [Document(page_content=split) for split in new_splits2]\n",
    "# save documents to files\n",
    "# for i, doc in enumerate(documents):\n",
    "#     with open(f\"docs/doc_{i}.txt\", \"w\") as f:\n",
    "#         f.write(doc.page_content)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "db = Chroma.from_documents(documents=documents, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"Snippet.{i+1}:\\n\\n{doc.page_content}\" for i, doc in enumerate(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = [\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "{\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\",\n",
    "},\n",
    "]\n",
    "genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "generation_config = {\n",
    "\"temperature\": 0.1,\n",
    "\"top_p\": 0.95,\n",
    "\"top_k\": 64,\n",
    "\"max_output_tokens\": 8192,\n",
    "\"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "model_name=\"gemini-1.5-pro\",\n",
    "generation_config=generation_config,\n",
    "safety_settings = safe\n",
    ")\n",
    "llm = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "def call_retrieval_sada(pretext, fifty_clean):\n",
    "\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    retrieved_docs = retriever.invoke(pretext)\n",
    "\n",
    "    formatted_context = combine_docs(retrieved_docs)\n",
    "\n",
    "    formatted_context_2 = \"Snippet.0: \\n\\n\" + fifty_clean + \"\\n\\n\" + formatted_context\n",
    "\n",
    "    return formatted_context_2\n",
    "\n",
    "    # return formatted_context, [], retrieved_docs, []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your cand sets below\n",
    "\n",
    "##### Add `Pretext`, `Fifty_Text` and `Query` as variables\n",
    "\n",
    "##### Original.txt, prompt, and sec_list must be in `manual_testing/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstart_chat(history\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_in_coverage.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "llm = model.start_chat(history=[])\n",
    "\n",
    "with open('prompt_in_coverage.txt', 'r') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "with open('sec_list.txt', 'r') as file:\n",
    "    sec_list = file.read()\n",
    "\n",
    "\n",
    "pretext = \"\"\"\n",
    "        }\n",
    "        ntemps -= tmp___2;\n",
    "        (files + out)->name = (char const *)(temp->name);\n",
    "        (files + out)->temp = temp;\n",
    "        in += num_merged;\n",
    "        out++;\n",
    "      }\n",
    "    while_break___0:\n",
    "      remainder = nfiles - in;\n",
    "      cheap_slots = (unsigned long)nmerge - out % (unsigned long)nmerge;\n",
    "      if (cheap_slots < remainder) {\n",
    "        nshortmerge = (remainder - cheap_slots) + 1UL;\n",
    "        tmp___3 = create_temp(&tfp___0);\n",
    "        temp___0 = tmp___3;\n",
    "        if (ntemps < nshortmerge) {\n",
    "          tmp___4 = ntemps;\n",
    "        } else {\n",
    "          tmp___4 = nshortmerge;\n",
    "        }\n",
    "        tmp___5 = mergefiles(files + in, tmp___4, nshortmerge, tfp___0, (char const *)(temp___0->name));\n",
    "        num_merged___0 = tmp___5;\n",
    "        if (ntemps < num_merged___0) {\n",
    "          tmp___6 = ntemps;\n",
    "        } else {\n",
    "          tmp___6 = num_merged___0;\n",
    "        }\n",
    "        ntemps -= tmp___6;\n",
    "        (files + out)->name = (char const *)(temp___0->name);\n",
    "        tmp___7 = out;\n",
    "        out++;\n",
    "        (files + tmp___7)->temp = temp___0;\n",
    "        in += num_merged___0;\n",
    "      }\n",
    "      memmove((void *)(files + out), (void const *)(files + in), (nfiles - in) * sizeof(*files));\n",
    "      ntemps += out;\n",
    "      nfiles -= in - out;\n",
    "    }\n",
    "  while_break:\n",
    "    avoid_trashing_input(files, ntemps, nfiles, output_file);\n",
    "    while (1) {\n",
    "      tmp___8 = open_input_files(files, nfiles, &fps);\n",
    "      nopened = tmp___8;\n",
    "      \n",
    "static void sort(char *const *files, size_t nfiles, char const *output_file, size_t nthreads) {\n",
    "  struct buffer buf___1;\n",
    "  size_t ntemps;\n",
    "  _Bool output_file_created;\n",
    "  char const *temp_output;\n",
    "  char const *file;\"\"\"\n",
    "\n",
    "fifty_clean = \"\"\"\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "  if (cheap_slots < remainder) {\n",
    "    nshortmerge = (remainder - cheap_slots) + 1UL;\n",
    "    tmp___3 = create_temp(&tfp___0);\n",
    "    temp___0 = tmp___3;\n",
    "    if (ntemps < nshortmerge) {\n",
    "      tmp___4 = ntemps;\n",
    "    } else {\n",
    "      tmp___4 = nshortmerge;\n",
    "    }\n",
    "    tmp___5 = mergefiles(files + in, tmp___4, nshortmerge, tfp___0,\n",
    "                         (char const *)(temp___0->name));\n",
    "    num_merged___0 = tmp___5;\n",
    "    if (ntemps < num_merged___0) {\n",
    "      tmp___6 = ntemps;\n",
    "    } else {\n",
    "      tmp___6 = num_merged___0;\n",
    "    }\n",
    "    ntemps -= tmp___6;\n",
    "    (files + out)->name = (char const *)(temp___0->name);\n",
    "    tmp___7 = out;\n",
    "    out++;\n",
    "    (files + tmp___7)->temp = temp___0;\n",
    "    in += num_merged___0;\n",
    "  }\n",
    "\"\"\"\n",
    "formatted_context = call_retrieval_sada(pretext, fifty_clean)\n",
    "\n",
    "prompt = prompt_template.format(sec_list=sec_list, formatted_context=formatted_context, query=query)\n",
    "\n",
    "response = llm.send_message(prompt).text\n",
    "\n",
    "print(\"\\nRESPONSE:\\n\",response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Required Functionality:\n",
      "\n",
      "**Functionality 1:** \n",
      "    * **Command:** `tar -cf archive.tar -C temp .`\n",
      "    * **Purpose:** Creates a new archive file named \"archive.tar\" in the current directory, containing all files and directories within the \"temp\" directory. \n",
      "    * **Reasoning:** This functionality is used in both test cases within the `run()` function to create an archive using the reduced binary.\n",
      "        * Test case 1: Compresses \"test1.txt\" into \"archive.tar\".\n",
      "        * Test case 2: Compresses \"test2.txt\", \"test3.bin\", and \"test4.txt\" into \"archive.tar\".\n",
      "\n",
      "**Parsing and formats:** The reduced binary needs to understand the `-cf` flags for creating an archive and specifying the filename. It also needs to correctly interpret the directory structure provided by `-C temp .` to include all necessary files and directories in the archive.\n",
      "\n",
      "\n",
      "## Unrequired Functionality (to be removed):\n",
      "\n",
      "1. **File Extraction (`-xf`)**: The provided script only utilizes the `REDUCED_BINARY` for creating archives (`-cf`). The extraction functionality (`-xf`) is solely used by `ORG_BINARY` for comparison and is not a requirement for the `REDUCED_BINARY`.\n",
      "2. **Listing Archive Contents (`-t`)**:  The script doesn't utilize the `-t` flag with `REDUCED_BINARY`. Listing archive contents is not a required functionality.\n",
      "3. **Other Operations**: Functionalities like `-d`, `-r`, `-u`, `-A`, etc., are not used with `REDUCED_BINARY` in the script and are therefore unnecessary.\n",
      "4. **Operation Modifiers**: Options like `-W`, `-k`, `--keep-newer-files`, `--overwrite`, etc., are not applied to `REDUCED_BINARY` within the script and can be considered unrequired.\n",
      "5. **Advanced Features**:  Features related to handling file attributes, device selection, archive format selection, local file selection, informative output, and compatibility options are not utilized with `REDUCED_BINARY` and are not required functionalities. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = model.start_chat(history=[])\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "# Run a command and capture its output\n",
    "\n",
    "c_program = \"tar-util.c\"\n",
    "test_oracle = \"tar.sh\"\n",
    "\n",
    "# c_program = \"chown-debloated.c\"\n",
    "# test_oracle = \"chown_train.sh\"\n",
    "\n",
    "# c_program = \"date-util.c\"\n",
    "# test_oracle = \"date.sh\"\n",
    "binary_name = c_program.split(\".\")[0]\n",
    "os.system(f'clang -w {c_program} -D __msan_unpoison\\(s,z\\) -lpcre -lpthread -o {binary_name}')\n",
    "\n",
    "output = subprocess.check_output([f\"\"\"./{binary_name}\"\"\", '--help'])\n",
    "functionality_list = output.decode('utf-8')\n",
    "\n",
    "with open('functionality_prompt.txt', 'r') as file:\n",
    "    func_template = file.read()\n",
    "\n",
    "with open(test_oracle, 'r') as file:\n",
    "    train_oracle = file.read()\n",
    "\n",
    "func_prompt = func_template.format(functionality_list = functionality_list, train_oracle=train_oracle)\n",
    "\n",
    "# print(func_prompt)\n",
    "# print(\"##################\")\n",
    "response = llm.send_message(func_prompt).text\n",
    "\n",
    "# response = re.sub(r\"\\*\\*Reasoning:\\*\\*.*\\n\", \"\", response)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are absolutely right! My apologies for the mistake. I incorrectly identified `-xf` decompression as a required functionality for the `REDUCED_BINARY`. \\n\\nThe `run()` function uses the `REDUCED_BINARY` only for the `-cf` compression operation. The `-xf` decompression is performed by the `ORG_BINARY` for comparison purposes. \\n\\nTherefore, the **required functionality** should only include:\\n\\n**Functionality:** `./tar-util -cf archive.tar -C temp .`\\n**Purpose:** Compresses files located in the `temp` directory into a tar archive named `archive.tar`.\\n**Reasoning:** This functionality is used in the `run()` function to compress the files in the `temp` directory using the `REDUCED_BINARY`.\\n\\nThe **unrequired functionality** remains the same as previously listed.\\n\\nI apologize for the confusion caused by my previous response. I am still under development and learning to better understand complex code structures. Thank you for pointing out my error! \\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_prompt = f\"\"\"Now your job is to analyse the program code for the utility given below, and identify which parts of this programs can be removed or should be retained to maintain the given Required and Unrequired Functionality.\n",
    "\n",
    "Program Code:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm.send_message(\"\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp___11 = mktime_ok((struct tm const *)(&tm0), (struct tm const *)(&tm), Start);\n",
      " if (!tmp___11) {\n",
      " goto fail;\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def merge_lines_with_semicolon(text):\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        if not current_line:\n",
    "            current_line = line\n",
    "        elif '=' in current_line and not current_line.strip().endswith(';'):\n",
    "            current_line += ' ' + stripped_line\n",
    "        else:\n",
    "            result.append(current_line)\n",
    "            current_line = line\n",
    "        \n",
    "        if current_line.strip().endswith(';'):\n",
    "            result.append(current_line)\n",
    "            current_line = \"\"\n",
    "    \n",
    "    if current_line:\n",
    "        result.append(current_line)\n",
    "    \n",
    "    return '\\n'.join(result)\n",
    "\n",
    "# Test the function\n",
    "text = '''tmp___11 =\n",
    " mktime_ok((struct tm const *)(&tm0), (struct tm const *)(&tm), Start);\n",
    " if (!tmp___11) {\n",
    " goto fail;\n",
    " }\n",
    "}'''\n",
    "\n",
    "result = merge_lines_with_semicolon(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

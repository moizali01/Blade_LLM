{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pysqlite3\n",
    "\n",
    "sys.modules['sqlite3'] = pysqlite3\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import ollama\n",
    "from datetime import datetime\n",
    "from functools import cached_property\n",
    "from langchain_community.llms import AzureOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader,TextLoader\n",
    "from langchain_text_splitters import (Language,RecursiveCharacterTextSplitter)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "import google.generativeai as genai\n",
    "from groq import Groq\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import requests\n",
    "import voyageai\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_voyageai import VoyageAIEmbeddings,VoyageAIRerank\n",
    "from tree_sitter_languages import get_language, get_parser\n",
    "from llama_index.core.text_splitter import CodeSplitter\n",
    "from dotenv import load_dotenv\n",
    "from chunker import get_code_chunks\n",
    "import subprocess\n",
    "\n",
    "\n",
    "dotenv_path = '../.env'\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM CLASS FOR THE WHOLE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "def doc_merger(splits):\n",
    "    current = 0\n",
    "    while True:\n",
    "        doc_lines = len(splits[current].splitlines())\n",
    "        if doc_lines < 3:\n",
    "            # merge with next doc\n",
    "            splits[current] += splits[current + 1]\n",
    "            splits.pop(current + 1)\n",
    "        else:\n",
    "            current += 1\n",
    "        \n",
    "        if current == len(splits) - 1:\n",
    "            return splits\n",
    "\n",
    "class QAClass:\n",
    "    # Class-level attributes\n",
    "    _embeddings = None\n",
    "    _db = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.timestamp = \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def initialize_embeddings_and_db(cls):\n",
    "        if cls._embeddings is None or cls._db is None:\n",
    "\n",
    "            ########### Voyage Code Embeddings ############\n",
    "\n",
    "            command = [\"clang-format\",\"-style={ColumnLimit: 300, AllowShortFunctionsOnASingleLine: All, AllowShortIfStatementsOnASingleLine: true}\",\"-i\",\"original.txt\"]\n",
    "\n",
    "            # Run the command\n",
    "            subprocess.run(command, check=True)\n",
    "\n",
    "            file_path = \"original.txt\"\n",
    "            with open(file_path, \"r\") as f:\n",
    "                docs = f.read()\n",
    "\n",
    "            splits = get_code_chunks(docs)\n",
    "            new_splits = [split for split in splits if len(split) > 2]\n",
    "            new_splits2 = doc_merger(new_splits)\n",
    "            # documents = [Document(page_content=split) for split in splits]\n",
    "            documents = [Document(page_content=split) for split in new_splits2]\n",
    "            os.environ['GOOGLE_API_KEY'] = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "            cls._embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "            cls._db = Chroma.from_documents(documents=documents, embedding=cls._embeddings)\n",
    "\n",
    "\n",
    "\n",
    "    # ********************************************\n",
    "    # LOCAL LLM FUNCTIONS ADDED HERE\n",
    "\n",
    "    def call_ollama(self, prompt):\n",
    "        response = requests.post('http://10.103.73.29:6970', json={'message': prompt})\n",
    "        # response = ollama.chat(model='llama3:8b-instruct-fp16', messages=[{'role': 'user', 'content': prompt}],options={\"temperature\":0.1})\n",
    "        # return response['message']['content']\n",
    "        return response.json()['response']\n",
    "\n",
    "    # gemini model definition and call\n",
    "    def call_gemini(self,prompt):\n",
    "\n",
    "        safe = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "        ]\n",
    "        genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "        generation_config = {\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "        model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "        safety_settings = safe\n",
    "        # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "        )\n",
    "        llm = model.start_chat(history=[])\n",
    "\n",
    "        response = None\n",
    "        try_count = 0\n",
    "        while try_count < 3:\n",
    "            try:\n",
    "                response = llm.send_message(prompt).text\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Error: \", e)\n",
    "                time.sleep(5)\n",
    "                try_count += 1\n",
    "                print(\"Retrying No. \", try_count)\n",
    "\n",
    "        return response     \n",
    "\n",
    "\n",
    "    def combine_docs(self, docs):\n",
    "        return \"\\n\\n\".join(f\"Snippet.{i+2}:\\n\\n{doc.page_content}\" for i, doc in enumerate(docs))\n",
    "\n",
    "    # os.environ[\"GROQ_API_KEY\"] = os.environ.get(\"GROQ_API_KEY\")\n",
    "    # def call_groq(self,prompt):\n",
    "    #         client = Groq(\n",
    "    #             api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    #         )\n",
    "    #         chat_completion = client.chat.completions.create(\n",
    "    #             temperature=0.1,\n",
    "    #             messages=[{\"role\": \"user\", \"content\": prompt},],\n",
    "    #             model=\"gemma-7b-it\",\n",
    "    #         )\n",
    "\n",
    "    #         return chat_completion.choices[0].message.content\n",
    "\n",
    "    # LOCAL LLM FUNCTIONS END HERE\n",
    "    # ********************************************\n",
    "\n",
    "    @cached_property\n",
    "    def llm_chain(self):\n",
    "\n",
    "        # ***** Model Initialization Starts Here *****\n",
    "        # Set up your LLM with API key Below (Not required for Ollama)\n",
    "\n",
    "        # 1. ***** OpenAI API (Azure AI Search) *****\n",
    "\n",
    "        # Setting up environment variables \n",
    "        # os.environ[\"OPENAI_API_TYPE\"] = \"Azure\"\n",
    "        # os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "        # os.environ[\"OPENAI_API_BASE\"] = \"https://debb.openai.azure.com/\"\n",
    "        # os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "        # # Suppressing warnings\n",
    "        # with warnings.catch_warnings():\n",
    "        #     warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        #     # Initialize AzureOpenAI model\n",
    "        #     llm = AzureOpenAI(\n",
    "        #         model_name=\"gpt-3.5-turbo-16k\",\n",
    "        #         model_kwargs={\"engine\": \"openaidemo-15999-16k\"},\n",
    "        #         temperature=0.2\n",
    "        #     )\n",
    "\n",
    "        # 2. ***** Gemini *****\n",
    "        llm = self.call_gemini\n",
    "\n",
    "        # 3. ***** Local LLM (Ollama) *****\n",
    "\n",
    "        # llm = self.call_ollama\n",
    "\n",
    "        # 4. ***** GROQQ CLOUD *****\n",
    "\n",
    "        # llm = self.call_groq\n",
    "\n",
    "        # ***** Model Initialization Ends Here *****\n",
    "\n",
    "        # Ensure embeddings and database are initialized\n",
    "        self.initialize_embeddings_and_db()\n",
    "\n",
    "        # Instead of creating a standard RetrievalQA chain, we'll use our custom rag_chain function\n",
    "        class CustomQAChain:\n",
    "            def __init__(self, outer_instance):\n",
    "                self.outer_instance = outer_instance\n",
    "\n",
    "            def run(self, query, context_query, coverage, fifty_clean):\n",
    "\n",
    "                # define retriever\n",
    "                retriever = self.outer_instance._db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "                # Use similarity_search_with_score instead of invoke\n",
    "                retrieved_docs_with_scores = self.outer_instance._db.similarity_search_with_score(context_query, k=4)\n",
    "\n",
    "                retrieved_docs = [doc for doc, _ in retrieved_docs_with_scores]\n",
    "                similarity_scores = [score for _, score in retrieved_docs_with_scores]\n",
    "\n",
    "\n",
    "                # Sort both lists based on similarity scores in descending order\n",
    "                sorted_pairs = sorted(zip(retrieved_docs, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Unzip the sorted pairs\n",
    "                retrieved_docs, similarity_scores = zip(*sorted_pairs)\n",
    "\n",
    "                # if len(query.splitlines()) > 10:\n",
    "                #     formatted_context_2 = self.outer_instance.combine_docs(retrieved_docs)\n",
    "                # else:\n",
    "                # formatted_context_2 = \"Snippet.0: \\n\\n\" + fifty_clean + \"\\n\\n\" + self.outer_instance.combine_docs(retrieved_docs[:2])\n",
    "                # Get the first 100 lines and last 100 lines\n",
    "                first_100_lines = \"\\n\".join(fifty_clean.splitlines()[:100])\n",
    "                last_100_lines = \"\\n\".join(fifty_clean.splitlines()[-100:])\n",
    "\n",
    "                # Create the new formatted_context_2\n",
    "                formatted_context_2 = (\n",
    "                    \"Snippet 0:\\n\\n\" + first_100_lines + \"\\n\\n\"\n",
    "                    \"Snippet 1:\\n\\n\" + last_100_lines + \"\\n\\n\"\n",
    "                    + self.outer_instance.combine_docs(retrieved_docs[:2])\n",
    "                )\n",
    "                \n",
    "                return self.outer_instance.call_llm(query, formatted_context_2, coverage, llm)\n",
    "\n",
    "\n",
    "        return CustomQAChain(self)\n",
    "    \n",
    "    # need to make three calls to LLM, 1) relevance of context, 2) generality and 3) security\n",
    "    def call_llm(self, query, formatted_context, coverage,llm):\n",
    "\n",
    "        # if coverage:\n",
    "        #     with open('prompt_in_coverage.txt', 'r') as file:\n",
    "        #         prompt_template = file.read()\n",
    "        # else:\n",
    "        #     with open(' prompt_not_in_coverage.txt', 'r') as file:\n",
    "        #         prompt_template = file.read()\n",
    "\n",
    "        # prompt = prompt_template.format(sec_list=sec_list, formatted_context=formatted_context, query=query)\n",
    "        # prompt_to_send = prompt\n",
    "\n",
    "        # Classify the context as relevant or not. If not, retain\n",
    "        # print(\"Checking Relevance\")\n",
    "        relevance = self.check_relevance(query, formatted_context, llm)\n",
    "        # print(relevance)\n",
    "        if relevance == \"no\":\n",
    "            return \"final verdict: class 4\"\n",
    "        else:\n",
    "            # query functionality\n",
    "            # print(\"Checking Functionality\")\n",
    "            functionality = self.check_functionality(query, formatted_context, llm, coverage)\n",
    "            # print(functionality)\n",
    "            # retain since it is needed for required functionality\n",
    "            if functionality == \"yes\":\n",
    "                return \"final verdict: class 4\"\n",
    "            else:\n",
    "                # query security\n",
    "                # print(\"Checking Security\")\n",
    "                security = self.check_security(query, formatted_context, llm, coverage)\n",
    "                # print(security)\n",
    "                # retain since it is needed for required security\n",
    "                if security == \"yes\":\n",
    "                    return \"final verdict: class 4\"\n",
    "                else:\n",
    "                    return \"final verdict: class 1\"\n",
    "                \n",
    "\n",
    "    # function to check relevance of context with query\n",
    "    def check_relevance(self, query, formatted_context, llm):\n",
    "        \n",
    "        with open(\"check_relevance_prompt.txt\", 'r') as file:\n",
    "            prompt_template = file.read()\n",
    "\n",
    "        prompt = prompt_template.format(context=formatted_context, query=query)\n",
    "\n",
    "        response = llm(prompt)\n",
    "\n",
    "        # save response to cands/multiagent/security\n",
    "        # file_name = \"cands/multiagent/relevance/relevance\" + \"_time_\"+ str(self.timestamp) + \".blade.c.txt\"\n",
    "        # with open(file_name, 'w') as file:\n",
    "        #     file.write(response)\n",
    "        # print(f\"Relevance Response: {response}\")\n",
    "        file_name = \"cands/multiagent/new_relevance/\" + self.timestamp\n",
    "        with open (file_name, 'w') as file:\n",
    "            file.write(response)\n",
    "\n",
    "\n",
    "        # parse response to get \"yes\" or \"no\"\n",
    "        match = re.search(r'\\b(yes|no)\\b', response.lower().strip())\n",
    "        if match is not None:\n",
    "            return match.group(1).lower()\n",
    "        else:\n",
    "            return \"no\"\n",
    "        # return \"yes\"\n",
    "\n",
    "    def check_functionality(self, query, formatted_context, llm, coverage):\n",
    "       \n",
    "        with open(\"functionality_explanation.txt\", 'r') as file:\n",
    "            functionality_explanation = file.read()\n",
    "            \n",
    "        prompt1 = functionality_explanation.format(context=formatted_context, query=query)\n",
    "        # print(f\"Functionality Explanation Prompt: \\n{prompt1}\")\n",
    "        # response1 = llm(prompt1)\n",
    "        response1 = llm.send_message(prompt1).text\n",
    "        # print(f\"Functionality Explanation Response: \\n{response1}\")\n",
    "        \n",
    "        \n",
    "        with open(\"functionality_prompt.txt\", 'r') as file:\n",
    "            prompt_template = file.read()\n",
    "\n",
    "        in_cov_statement = \"This code snippet included in the code execution path for the required functionality, therefore verify if the given code snippet is important for required functionality of the program.\"\n",
    "        not_cov_statement = \"This code snippet is not included in the code execution path for the required functionality, therefore verify if the given code snippet is important for the required functionality of the program.\"\n",
    "\n",
    "        with open(\"req_list.txt\", 'r') as file:\n",
    "            req_list = file.read()\n",
    "\n",
    "        cov_info = in_cov_statement if coverage else not_cov_statement\n",
    "\n",
    "        # prompt = prompt_template.format(context=formatted_context, query=query, coverage_info=cov_info, req_list=req_list)\n",
    "        prompt = prompt_template.format(query=response1, coverage_info=cov_info, req_list=req_list)\n",
    "        # print(f\"Functionality Prompt: \\n{prompt}\")\n",
    "\n",
    "        # response = llm(prompt)\n",
    "        response = llm.send_message(prompt).text\n",
    "\n",
    "        # save response to cands/multiagent/functionality\n",
    "        # file_name = \"cands/multiagent/functionality/functionality\" + \"_time_\"+ str(self.timestamp) + \".blade.c.txt\"\n",
    "        # with open(file_name, 'w') as file:\n",
    "        #     file.write(response)\n",
    "        # print(f\"Functionality Response:\\n{response}\")\n",
    "        filename = \"cands/multiagent/new_functionality/\" + self.timestamp\n",
    "        combined_response = response1 + \"\\n\\n\" + response\n",
    "        with open (filename, 'w') as file:\n",
    "            file.write(combined_response)\n",
    "            \n",
    "\n",
    "        # parse response to get \"yes\" or \"no\"\n",
    "        match = self.extract_imp_score_new_prompt(response)\n",
    "        if match is not None:\n",
    "            if match == 9:\n",
    "                return \"yes\"\n",
    "            elif match == 1:\n",
    "                return \"no\"\n",
    "\n",
    "        # retain if there is any issue in extracting importance score\n",
    "        return \"yes\"\n",
    "\n",
    "    def check_security(self, query, formatted_context, llm, coverage):\n",
    "\n",
    "        if os.path.exists(\"sec_list.txt\"):\n",
    "            with open(\"sec_list.txt\", 'r') as file:\n",
    "                sec_list = file.read()\n",
    "            # print(\"sec_list found and read\")\n",
    "        else:\n",
    "            sec_list = self.get_security_checks()\n",
    "            print(sec_list)\n",
    "            print(\"Sec List Read\")\n",
    "\n",
    "        with open(\"security_prompt.txt\", 'r') as file:\n",
    "            prompt_template = file.read()\n",
    "\n",
    "        in_cov_statement = \"This code snippet included in the code execution path for the required functionality, therefore verify if the given code snippet is important for any of the listed potential security vulnerabilities in the program.\"\n",
    "        not_cov_statement = \"This code snippet is not included in the code execution path for the required functionality, therefore verify if the given code snippet is important for any of the listed potential security vulnerabilities in the program.\"\n",
    "\n",
    "        cov_info = in_cov_statement if coverage else not_cov_statement\n",
    "\n",
    "        prompt = prompt_template.format(context=formatted_context, query=query, coverage_info=cov_info, sec_list=sec_list)\n",
    "\n",
    "        response = llm(prompt)\n",
    "\n",
    "        # save response to cands/multiagent/security\n",
    "        # file_name = \"cands/multiagent/security/security\" + \"_time_\"+ str(self.timestamp) + \".blade.c.txt\"\n",
    "        # with open(file_name, 'w') as file:\n",
    "        #     file.write(response)\n",
    "        # print(f\"Security Response: {response}\")\n",
    "        file_name = \"cands/multiagent/new_security/\" + self.timestamp\n",
    "        with open (file_name, 'w') as file:\n",
    "            file.write(response)\n",
    "\n",
    "        # parse response to get \"yes\" or \"no\"\n",
    "        match = self.extract_imp_score_new_prompt(response)\n",
    "        if match is not None:\n",
    "            if match == 9:\n",
    "                return \"yes\"\n",
    "            elif match == 1:\n",
    "                return \"no\"\n",
    "\n",
    "        # retain if there is any issue in extracting importance score\n",
    "        return \"yes\"\n",
    "\n",
    "    def invoke(self, query, prompt_type, context_query, fifty_clean, passed_time):\n",
    "        # Retrieve documents based on context query\n",
    "        # print(\"Querying: \", query)\n",
    "\n",
    "        self.timestamp = passed_time\n",
    "\n",
    "        if prompt_type == 'generality':\n",
    "            return self.llm_chain.run(query, context_query, False, fifty_clean)\n",
    "        elif prompt_type == 'security':\n",
    "            return self.llm_chain.run(query, context_query, True, fifty_clean)\n",
    "        \n",
    "\n",
    "    def get_security_checks(self):\n",
    "        code_path = \"original.txt\"\n",
    "        with open(code_path, 'r') as file:\n",
    "            code = file.read()\n",
    "\n",
    "        with open(\"req_list.txt\", 'r') as file:\n",
    "            req_list = file.read()\n",
    "\n",
    "        sec_prompt = f\"\"\"The program to analyze is a core Linux utility. Your task is to analyze the provided program code and the list of **required functionality** to generate a concise list of potential security vulnerabilities that could affect the required functionality. \n",
    "\n",
    "### Instructions:\n",
    "- Consider security and exception-related issues such as buffer overflows, race conditions, input validation issues, improper use of system calls, privilege escalation, and resource management vulnerabilities.\n",
    "- Focus only on vulnerabilities that could impact the **required functionality**. \n",
    "- Address security concerns related to **input validation**, **boundary checking**, **memory management**, and **handling of special file types**.\n",
    "- Do not include points related to excluded or unrequired functionality.\n",
    "- **Do not** discuss generality, flags, or unrelated features of the program.\n",
    "- Ensure the list is limited to a maximum of **20 concise bullet points**.\n",
    "\n",
    "### Reference Information:\n",
    "{req_list}\n",
    "\n",
    "- **Program Code**: \n",
    "########\n",
    "{code}\n",
    "########\n",
    "\n",
    "### Output:\n",
    "- Provide a list of bullet points, each describing a potential security vulnerability or exception that should be tested in the context of the **required functionality** only, including issues related to input validation, memory usage, boundary checks, race conditions, and resource handling. Make sure the list is concise and limited to a maximum of 20 bullet points.\"\"\"      \n",
    "        safe = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "        ]\n",
    "        # genai.configure(api_key=\"AIzaSyCeL2G0fQvkgYn95s7p0orgbgOqtO-lZ28\")\n",
    "        genai.configure(api_key=os.environ.get(\"GENAI_API_KEY\"))\n",
    "        generation_config = {\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.95,\n",
    "        \"top_k\": 64,\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"response_mime_type\": \"text/plain\",\n",
    "        }\n",
    "        model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "        safety_settings = safe\n",
    "        # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "        )\n",
    "        llm_sec = model.start_chat(history=[])\n",
    "\n",
    "        ###\n",
    "        sec_list = (llm_sec.send_message(sec_prompt)).text\n",
    "\n",
    "        with open(\"sec_list.txt\", 'w') as file:\n",
    "            file.write(sec_list)\n",
    "\n",
    "        return sec_list\n",
    "\n",
    "\n",
    "    def extract_imp_score_new_prompt(self,text): \n",
    "\n",
    "        # Convert the text to lowercase\n",
    "        lower_text = text.lower()\n",
    "        target = \"final verdict\"\n",
    "\n",
    "        # Find the position of \"importance score\"\n",
    "        start_index = lower_text.find(target)\n",
    "        if start_index == -1:\n",
    "            return None  # \"Importance Score\" not found\n",
    "\n",
    "        # Move the start index to the end of \"importance score\"\n",
    "        start_index += len(target)\n",
    "\n",
    "        # Find the next newline character after \"importance score\"\n",
    "        end_index = text.find('\\n', start_index)\n",
    "        if end_index == -1:\n",
    "            end_index = len(text)  # if no newline, go to the end of the text\n",
    "\n",
    "        # Extract the substring between \"importance score\" and the newline\n",
    "        substring = text[start_index:end_index]\n",
    "\n",
    "        # Find the number in the substring\n",
    "        import re\n",
    "        match = re.search(r'\\b([0-9]|[1-9][0-9]|100)\\b', substring)\n",
    "        if match:\n",
    "            assigned_class = int(match.group(0))\n",
    "            # removal class\n",
    "            if assigned_class < 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return 9\n",
    "        else:\n",
    "            return None  # No number found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = QAClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example template if you want to check a single candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\"\"\"\n",
    "\n",
    "prompt_type = \"security\"\n",
    "\n",
    "context_query = \"\"\"\"\"\"\n",
    "\n",
    "fifty_clean = \"\"\"\"\"\"\n",
    "passed_time = \"2001\"\n",
    "\n",
    "# qa.invoke(query, prompt_type, context_query, fifty_clean, passed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun on all the candidate sets to recalculate benchmark scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/522 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  block_reason: OTHER\n",
      "\n",
      "Retrying No.  1\n",
      "Error:  block_reason: OTHER\n",
      "\n",
      "Retrying No.  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  21%|██        | 110/522 [00:15<00:56,  7.33file/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m exist_in_coverage \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(context_dir, file))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exist_in_coverage:        \n\u001b[0;32m---> 49\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremoved_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msecurity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfifty_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39minvoke(removed_code, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerality\u001b[39m\u001b[38;5;124m\"\u001b[39m, context_query, fifty_clean, file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 424\u001b[0m, in \u001b[0;36mQAClass.invoke\u001b[0;34m(self, query, prompt_type, context_query, fifty_clean, passed_time)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mrun(query, context_query, \u001b[38;5;28;01mFalse\u001b[39;00m, fifty_clean)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m prompt_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecurity\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfifty_clean\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 215\u001b[0m, in \u001b[0;36mQAClass.llm_chain.<locals>.CustomQAChain.run\u001b[0;34m(self, query, context_query, coverage, fifty_clean)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Create the new formatted_context_2\u001b[39;00m\n\u001b[1;32m    209\u001b[0m formatted_context_2 \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnippet 0:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m first_100_lines \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnippet 1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m last_100_lines \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter_instance\u001b[38;5;241m.\u001b[39mcombine_docs(retrieved_docs[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    213\u001b[0m )\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_context_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctionality_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecurity_llm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 235\u001b[0m, in \u001b[0;36mQAClass.call_llm\u001b[0;34m(self, query, formatted_context, coverage, classifier_llm, functionality_llm, security_llm)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_llm\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, formatted_context, coverage, classifier_llm, functionality_llm, security_llm):\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# if coverage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Classify the context as relevant or not. If not, retain\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# print(\"Checking Relevance\")\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# print(relevance)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relevance \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[26], line 267\u001b[0m, in \u001b[0;36mQAClass.check_relevance\u001b[0;34m(self, query, formatted_context, llm)\u001b[0m\n\u001b[1;32m    263\u001b[0m     prompt_template \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    265\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mformatted_context, query\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m--> 267\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# save response to cands/multiagent/security\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# file_name = \"cands/multiagent/relevance/relevance\" + \"_time_\"+ str(self.timestamp) + \".blade.c.txt\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# with open(file_name, 'w') as file:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m#     file.write(response)\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# print(f\"Relevance Response: {response}\")\u001b[39;00m\n\u001b[1;32m    274\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcands/multiagent/new_relevance/ \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp\n",
      "Cell \u001b[0;32mIn[26], line 104\u001b[0m, in \u001b[0;36mQAClass.call_gemini\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m try_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/generativeai/generative_models.py:578\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid configuration: The chat functionality does not support `candidate_count` greater than 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n\u001b[0;32m--> 578\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:827\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:400\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:62\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:58\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._interpret_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi:71\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._BatchOperationTag.event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi:138\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:69\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadata\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:70\u001b[0m, in \u001b[0;36mgenexpr\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:64\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadatum\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, key, value)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from exist_coverage import exist\n",
    "from tqdm import tqdm\n",
    "cands_dir = \"cands\"\n",
    "context_dir = cands_dir + \"/context\"\n",
    "fiftytext_dir = cands_dir + \"/fifty_text\"\n",
    "pretext_dir = cands_dir + \"/pretext_code\"\n",
    "removed_code_dir = cands_dir + \"/removed_code\"\n",
    "\n",
    "output_llm_dir = cands_dir + \"/multiagent/output_llm\"\n",
    "    \n",
    "# make sure output dirs exists\n",
    "if not os.path.exists(\"cands/multiagent\"):\n",
    "    os.makedirs(\"cands/multiagent\")\n",
    "if not os.path.exists(output_llm_dir):\n",
    "    os.makedirs(output_llm_dir)\n",
    "if not os.path.exists(\"cands/multiagent/new_relevance\"):\n",
    "    os.makedirs(\"cands/multiagent/new_relevance\")\n",
    "if not os.path.exists(\"cands/multiagent/new_security\"):\n",
    "    os.makedirs(\"cands/multiagent/new_security\")\n",
    "if not os.path.exists(\"cands/multiagent/new_functionality\"):\n",
    "    os.makedirs(\"cands/multiagent/new_functionality\")\n",
    "\n",
    "# Get the total number of files to process\n",
    "context_files = [f for f in os.listdir(context_dir) if os.path.isfile(os.path.join(context_dir, f))] #you can sample from these if you want an estimate only\n",
    "total_files = len(context_files)\n",
    "\n",
    "# Create a single tqdm progress bar\n",
    "with tqdm(total=total_files, desc=\"Processing files\", unit=\"file\") as pbar:\n",
    "    for file in context_files:\n",
    "        # if file exists in cands/multiagent/output_llm, skip\n",
    "        if os.path.exists(os.path.join(output_llm_dir, os.path.basename(file).replace(\"context_\", \"llm_\") + \".txt\")):\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        context_query = \"\"\n",
    "        with open(os.path.join(context_dir, file), 'r') as f:\n",
    "            context_query = f.read()\n",
    "        fifty_clean = \"\"\n",
    "        fifty_filename = \"fifty_\" + os.path.basename(file)\n",
    "        with open(os.path.join(fiftytext_dir, fifty_filename), 'r') as f:\n",
    "            fifty_clean = f.read()\n",
    "        pretext_code = \"\"\n",
    "        with open(os.path.join(pretext_dir, file), 'r') as f:\n",
    "            pretext_code = f.read()\n",
    "        removed_code = context_query.strip()\n",
    "        \n",
    "        exist_in_coverage = os.path.exists(os.path.join(context_dir, file))\n",
    "        if exist_in_coverage:        \n",
    "            llm_response = qa.invoke(removed_code, \"security\", context_query, fifty_clean, file + \".txt\")\n",
    "        else:\n",
    "            llm_response = qa.invoke(removed_code, \"generality\", context_query, fifty_clean, file + \".txt\")\n",
    "            \n",
    "        # save the response to the output directory\n",
    "        llm_filename = os.path.join(output_llm_dir, os.path.basename(file).replace(\"context_\", \"llm_\") + \".txt\")\n",
    "        with open(llm_filename, 'w') as f:\n",
    "            f.write(llm_response)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "Classifies into different folders in the output directory and returns a benchmark statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Retention: 75\n",
      "False Removal: 5\n",
      "True Retention: 26\n",
      "True Removal: 4\n",
      "Total: 110\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Function to get the line numbers of the code snippet in the context file\n",
    "def get_line_numbers(file_content):\n",
    "    lines = file_content.splitlines()\n",
    "    snippet_line_numbers = [i + 1 for i, line in enumerate(lines) if line.strip() != '']\n",
    "    return snippet_line_numbers\n",
    "\n",
    "# Function to extract the LLM response using the provided regex\n",
    "def extract_imp_score_new_prompt(text):\n",
    "    lower_text = text.lower()\n",
    "    target = \"final verdict\"\n",
    "\n",
    "    start_index = lower_text.find(target)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    start_index += len(target)\n",
    "    end_index = text.find('\\n', start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(text)\n",
    "\n",
    "    substring = text[start_index:end_index]\n",
    "    match = re.search(r'\\b([0-9]|[1-9][0-9]|100)\\b', substring)\n",
    "    if match:\n",
    "        assigned_class = int(match.group(0))\n",
    "        if assigned_class < 3:\n",
    "            return 1\n",
    "        else:\n",
    "            return 9\n",
    "    return None\n",
    "\n",
    "# Function to check if the line starts with a comment (ignoring whitespace) in the gold file\n",
    "def check_gold_label_line(gold_file_path, line_number):\n",
    "    try:\n",
    "        with open(gold_file_path, 'r') as gold_file:\n",
    "            lines = gold_file.readlines()\n",
    "            if line_number <= len(lines):\n",
    "                target_line = lines[line_number - 1].strip()\n",
    "                if target_line.startswith(\"//\"):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading gold file {gold_file_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "# Function to create directories for each category\n",
    "def create_category_directories(base_output_folder):\n",
    "    categories = ['false_retention', 'false_removal', 'true_retention', 'true_removal']\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(base_output_folder, category), exist_ok=True)\n",
    "    os.makedirs(incomplete_context_folder, exist_ok=True)\n",
    "    # print(\"Folder made:\", incomplete_context_folder)\n",
    "\n",
    "\n",
    "def create_merged_file(context_content, llm_content, time_stamp, category_folder, fifty_text_folder, pretext_code_folder, relevance_code_folder, functionality_code_folder, security_code_folder, incomplete_context_folder):\n",
    "    context_content = context_content.strip()\n",
    "    merged_file_name = f\"merged_time_{time_stamp}.txt\"\n",
    "    merged_file_path = os.path.join(category_folder, merged_file_name)\n",
    "\n",
    "    # Define the corresponding filenames for fifty_text and pretext_code\n",
    "    fifty_text_file = os.path.join(fifty_text_folder, f\"fifty_context_{time_stamp}\")\n",
    "    pretext_code_file = os.path.join(pretext_code_folder, f\"context_{time_stamp}\")\n",
    "    relevance_code_file = os.path.join(relevance_code_folder, f\"context_{time_stamp}\" + \".txt\")\n",
    "    functionality_code_file = os.path.join(functionality_code_folder, f\"context_{time_stamp}\" + \".txt\")\n",
    "    security_code_file = os.path.join(security_code_folder, f\"context_{time_stamp}\" + \".txt\")\n",
    "    incomplete_context_file = os.path.join(incomplete_context_folder, merged_file_name)\n",
    "    \n",
    "    \n",
    "    # Read the content from the fifty_text file if it exists\n",
    "    fifty_text_content = \"\"\n",
    "    # if len(context_content.splitlines()) < 10:\n",
    "        # print(f\"Length of context content is {len(context_content.splitlines())}, filename: {fifty_text_file}\")  \n",
    "    if os.path.exists(fifty_text_file):\n",
    "        with open(fifty_text_file, 'r') as file:\n",
    "            fifty_text_content = file.read().strip()\n",
    "    else:\n",
    "        print(f\"Fifty text file not found for {fifty_text_file}\")\n",
    "    # else:\n",
    "    #     print(f\"Lenght of context content is {len(context_content)}, filename: {fifty_text_file}\")\n",
    "\n",
    "    # Read the content from the pretext_code file if it exists\n",
    "    pretext_code_content = \"\"\n",
    "    if os.path.exists(pretext_code_file):\n",
    "        with open(pretext_code_file, 'r') as file:\n",
    "            pretext_code_content = file.read().strip()\n",
    "        # print(f\"Pretext code file found for {pretext_code_file}\")\n",
    "    # else:\n",
    "    #     print(f\"Pretext code file not found for {pretext_code_file}\")        \n",
    "    \n",
    "    relevance_code_content = \"\"\n",
    "    if os.path.exists(relevance_code_file):\n",
    "        with open(relevance_code_file, 'r') as file:\n",
    "            relevance_code_content = file.read().strip()\n",
    "        \n",
    "    functionality_code_content = \"\"\n",
    "    if os.path.exists(functionality_code_file):\n",
    "        with open(functionality_code_file, 'r') as file:\n",
    "            functionality_code_content = file.read().strip()\n",
    "\n",
    "    security_code_content = \"\"\n",
    "    if os.path.exists(security_code_file):\n",
    "        with open(security_code_file, 'r') as file:\n",
    "            security_code_content = file.read().strip()\n",
    "\n",
    "\n",
    "    if not functionality_code_content:\n",
    "        with open(incomplete_context_file, 'w') as ctx_file:\n",
    "            ctx_file.write(context_content)\n",
    "            ctx_file.write(\"\\n\\Relevance response:\\n\\n\")\n",
    "            ctx_file.write(relevance_code_content)\n",
    "            if pretext_code_content:\n",
    "                ctx_file.write(\"\\n\\nPretext Code:\\n\\n\")\n",
    "                ctx_file.write(pretext_code_content)\n",
    "            \n",
    "            if fifty_text_content:\n",
    "                ctx_file.write(\"\\n\\nFifty Text:\\n\\n\")\n",
    "                ctx_file.write(fifty_text_content)\n",
    "            \n",
    "    else:\n",
    "        # Write the merged file with all content\n",
    "        with open(merged_file_path, 'w') as merged_file:\n",
    "            merged_file.write(\"Query:\\n\")\n",
    "            merged_file.write(context_content)\n",
    "            \n",
    "            merged_file.write(\"\\n\\nLLM Response:\\n\\n\")\n",
    "            merged_file.write(llm_content)\n",
    "            \n",
    "            if functionality_code_content:\n",
    "                merged_file.write(\"\\n\\nRelevance Code:\\n\\n\")\n",
    "                merged_file.write(relevance_code_content)\n",
    "                \n",
    "                merged_file.write(\"\\n\\nFunctionality Code:\\n\\n\")\n",
    "                merged_file.write(functionality_code_content)\n",
    "                \n",
    "                merged_file.write(\"\\n\\nSecurity Code:\\n\\n\")\n",
    "                merged_file.write(security_code_content)                  \n",
    "                \n",
    "            if pretext_code_content:\n",
    "                merged_file.write(\"\\n\\nPretext Code:\\n\\n\")\n",
    "                merged_file.write(pretext_code_content)\n",
    "            \n",
    "            if fifty_text_content:\n",
    "                merged_file.write(\"\\n\\nFifty Text:\\n\\n\")\n",
    "                merged_file.write(fifty_text_content)\n",
    "\n",
    "\n",
    "    # print(f\"Merged file created: {merged_file_path}\")\n",
    "\n",
    "\n",
    "# Main script\n",
    "def process_context_files(context_folder, llm_response_folder, gold_label_path, base_output_folder):\n",
    "    context_files = glob.glob(os.path.join(context_folder, \"*.c\"))\n",
    "    create_category_directories(base_output_folder)  # Create necessary directories\n",
    "    false_retention = 0\n",
    "    false_removal = 0\n",
    "    true_retention = 0\n",
    "    true_removal = 0 \n",
    "    \n",
    "    for context_file in context_files:\n",
    "        # try:\n",
    "            # Read the context file content\n",
    "            with open(context_file, 'r') as file:\n",
    "                context_data = file.read()\n",
    "\n",
    "            # Get line numbers where code snippets exist\n",
    "            line_numbers = get_line_numbers(context_data)\n",
    "\n",
    "            # Derive the corresponding LLM response filename from the context filename\n",
    "            base_filename = os.path.basename(context_file).replace(\"context_\", \"llm_\") + \".txt\"\n",
    "            llm_response_file = os.path.join(llm_response_folder, base_filename)\n",
    "\n",
    "            if os.path.exists(llm_response_file):\n",
    "                # Read the LLM response file\n",
    "                with open(llm_response_file, 'r') as llm_file:\n",
    "                    llm_data = llm_file.read()\n",
    "\n",
    "                # Extract importance score from the LLM response\n",
    "                imp_score = extract_imp_score_new_prompt(llm_data)\n",
    "\n",
    "                # For each line number, check the corresponding gold label line\n",
    "                is_comment = False\n",
    "                for line_number in line_numbers:\n",
    "                    is_comment = check_gold_label_line(gold_label_path, line_number)\n",
    "                    if is_comment:\n",
    "                        break\n",
    "\n",
    "                # Generate a time stamp for file naming\n",
    "                time_stamp = base_filename.replace(\"llm_\", \"\").replace(\".txt\", \"\")\n",
    "\n",
    "                # Determine the category and update counts\n",
    "                if imp_score > 5:\n",
    "                    if is_comment:\n",
    "                        category = \"false_retention\"\n",
    "                        false_retention += 1\n",
    "                    else:\n",
    "                        category = \"true_retention\"\n",
    "                        true_retention += 1\n",
    "                else:\n",
    "                    if is_comment:\n",
    "                        category = \"true_removal\"\n",
    "                        true_removal += 1\n",
    "                    else:\n",
    "                        category = \"false_removal\"\n",
    "                        false_removal += 1\n",
    "\n",
    "                # Create the merged response file in the appropriate category folder\n",
    "                create_merged_file(context_data, llm_data, time_stamp, os.path.join(base_output_folder, category), fifty_text_folder, pretext_code_folder, relevance_code_folder, functionality_code_folder, security_code_folder, incomplete_context_folder)\n",
    "\n",
    "            else:\n",
    "                # print(f\"LLM response file not found for {base_filename}\")\n",
    "                continue\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error processing file {context_file}: {e}\")\n",
    "            \n",
    "    print(f\"False Retention: {false_retention}\")\n",
    "    print(f\"False Removal: {false_removal}\")\n",
    "    print(f\"True Retention: {true_retention}\")\n",
    "    print(f\"True Removal: {true_removal}\")\n",
    "    print(f\"Total: {false_retention + false_removal + true_retention + true_removal}\")\n",
    "\n",
    "# Example usage:\n",
    "context_folder = \"cands/context\"\n",
    "llm_response_folder = \"cands/multiagent/output_llm\"\n",
    "fifty_text_folder = \"cands/fifty_text\"\n",
    "pretext_code_folder = \"cands/pretext_code\"\n",
    "relevance_code_folder = \"cands/multiagent/new_relevance\"\n",
    "functionality_code_folder = \"cands/multiagent/new_functionality\"\n",
    "security_code_folder = \"cands/multiagent/new_security\"\n",
    "gold_label_path = \"uniq-goldlabel.c\"  # Update with your actual gold label file path\n",
    "base_output_folder = \"output\"  # Base folder for the output merged responses\n",
    "incomplete_context_folder = \"output/false_retention/incomplete_context\"\n",
    "\n",
    "process_context_files(context_folder, llm_response_folder, gold_label_path, base_output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
